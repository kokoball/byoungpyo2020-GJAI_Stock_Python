{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종목별 핫이슈 키워드 전처리 + 키워드 기반 종목 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from konlpy.tag import Okt\n",
    "from math import log, exp\n",
    "import collections\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('종목별 핫이슈 키워드.csv')\n",
    "\n",
    "# 필요한 컬럼만 사용\n",
    "data = df[['ITMT_CD_NM','ISSUE_CTNT']]\n",
    "\n",
    "#  전처리\n",
    "data['ISSUE_CTNT'][0][1:-1]\n",
    "\n",
    "def preprocess(str_list):\n",
    "    p = re.compile(\"'.+?'\")\n",
    "    matched = p.findall(str_list)\n",
    "\n",
    "    w_list = []\n",
    "    for w in matched:\n",
    "        kor = w.replace(\"'\",'')\n",
    "        if kor != 'words' and kor != 'wordweights':\n",
    "            w_list.append(kor)\n",
    "    \n",
    "    return w_list\n",
    "\n",
    "kor_data = []\n",
    "for w in data['ISSUE_CTNT']:\n",
    "    kor_list = preprocess(w[1:-1])\n",
    "    kor_data.append(kor_list)\n",
    "\n",
    "df1 = pd.DataFrame({'words': kor_data})\n",
    "df2 = pd.DataFrame({'name': data['ITMT_CD_NM']})\n",
    "df_keyword = pd.merge(df2, df1, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 인코더 디코더 설정\n",
    "encoder_input, decoder_input, decoder_output = [], [], []\n",
    "\n",
    "for word in df_keyword['words']:\n",
    "    encoder_input.append(word)\n",
    "\n",
    "for name in df_keyword['name']:\n",
    "    decoder_input.append((\"<start> \" + name).split())\n",
    "\n",
    "for name in df_keyword['name']:\n",
    "    decoder_output.append((name+\" <end>\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['임상', '미', '약품', '스펙트럼', '코호트', '신약', '포지오', '티', '환자', '평가', '약품', '미', '제약', '에페', '글', '타이드', '레나', '사노', '피', '경제', '약품', '미', '탄', '신약', '잘탄', '패밀리', '매출', '아모', '개발', '경제', '임상', '기술', '바이오', '제약', '치료제', '밝혔', '받', '환자', '평가', '병원', '약품', '미', '사노', '피', '글', '에페', '레나', '타이드', '경제', '개발'], ['정세균', '중공업', '수산', '관련', '알루코', '한경', '총리', '경제', '주가', '급등', '수산', '중공업', '배', '경제', '서울', 'PER', 'PBR', '업종', '차트', '정배', '분기', '이익', '영업', '매출액', '한국', '중공업', '수산', '경제', '흑자', '전환'], ['수송관', '시험', '성능', '한난', '센터', '경제', '한국지역난방공사', '안전', '사업', '사장']]\n",
      "[['<start>', '한미약품'], ['<start>', '수산중공업'], ['<start>', '지역난방공사']]\n",
      "[['한미약품', '<end>'], ['수산중공업', '<end>'], ['지역난방공사', '<end>']]\n"
     ]
    }
   ],
   "source": [
    "# 확인\n",
    "print(encoder_input[:3])\n",
    "print(decoder_input[:3])\n",
    "print(decoder_output[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리의 데이터는 이미 단어별로 나누어져있음\n",
    "# 단어 토큰화 생략, but 정수 인코딩은 필요\n",
    "\n",
    "# fit_on_texts : 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성\n",
    "\n",
    "tokenizer_ko = Tokenizer()\n",
    "tokenizer_ko.fit_on_texts(encoder_input)\n",
    "encoder_input = tokenizer_ko.texts_to_sequences(encoder_input)\n",
    "\n",
    "tokenizer_ko.fit_on_texts(decoder_input)\n",
    "tokenizer_ko.fit_on_texts(decoder_output)\n",
    "\n",
    "decoder_input = tokenizer_ko.texts_to_sequences(decoder_input)\n",
    "decoder_output = tokenizer_ko.texts_to_sequences(decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[62, 276, 191, 6083, 6272, 256, 6084, 126, 202, 227, 191, 276, 60, 3751, 456, 2945, 3702, 3105, 307, 1, 191, 276, 1486, 256, 6836, 4510, 38, 3427, 27, 1, 62, 48, 44, 60, 80, 1161, 67, 202, 227, 480, 191, 276, 3105, 307, 456, 3751, 3702, 2945, 1, 27], [7669, 128, 2424, 335, 9325, 9, 2633, 1, 21, 1942, 2424, 128, 24, 1, 58, 55, 47, 96, 14, 75, 5, 10, 4, 31, 2, 128, 2424, 1, 189, 125], [4912, 352, 1893, 3311, 165, 1, 2654, 284, 11, 138]]\n",
      "[[2, 1391], [2, 4549], [2, 1712]]\n",
      "[[1391, 3], [4549, 3], [1712, 3]]\n"
     ]
    }
   ],
   "source": [
    "# 확인\n",
    "print(encoder_input[:3])\n",
    "print(decoder_input[:3])\n",
    "print(decoder_output[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 패딩을 하기 위해 encoderd의 길이 파악\n",
    "len_ko = []\n",
    "for data in encoder_input:\n",
    "    len_ko.append(len(data))\n",
    "\n",
    "plt.hist(len_ko, label='ko', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩을 수행\n",
    "# 기본적으론 앞에 0를 붙이는데, post를 쓰면 뒤에 패딩이 됨\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "\n",
    "# 디코더는 길이가 다 2이므로 패딩이 필요없다.\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_output = pad_sequences(decoder_output, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  62  276  191 6083 6272  256 6084  126  202  227  191  276   60 3751\n",
      "   456 2945 3702 3105  307    1  191  276 1486  256 6836 4510   38 3427\n",
      "    27    1   62   48   44   60   80 1161   67  202  227  480  191  276\n",
      "  3105  307  456 3751 3702 2945    1   27]\n",
      " [7669  128 2424  335 9325    9 2633    1   21 1942 2424  128   24    1\n",
      "    58   55   47   96   14   75    5   10    4   31    2  128 2424    1\n",
      "   189  125    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [4912  352 1893 3311  165    1 2654  284   11  138    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n",
      "[[   2 1391    0]\n",
      " [   2 4549    0]\n",
      " [   2 1712    0]]\n",
      "[[1391    3    0]\n",
      " [4549    3    0]\n",
      " [1712    3    0]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[:3])\n",
    "print(decoder_input[:3])\n",
    "print(decoder_output[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나중에 prediction 할때 사용하기 위함 (인덱스로 단어 찾기)\n",
    "en_to_index = tokenizer_ko.word_index\n",
    "index_to_en = tokenizer_ko.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3:1 비율로 test_size와 train_size 설정\n",
    "test_size = 62000\n",
    "encoder_input_train = encoder_input[:-test_size]\n",
    "decoder_input_train = decoder_input[:-test_size]\n",
    "decoder_output_train = decoder_output[:-test_size]\n",
    "\n",
    "encoder_input_test = encoder_input[-test_size:]\n",
    "decoder_input_test = decoder_input[-test_size:]\n",
    "decoder_output_test = decoder_output[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 모델\n",
    "encoder_inputs = Input(shape=(50,)) \n",
    "encoder_embed = Embedding(len(tokenizer_ko.word_index)+1, 50)(encoder_inputs)\n",
    "encoder_mask = Masking(mask_value=0)(encoder_embed)\n",
    "encoder_outputs, h_state, c_state = LSTM(50, return_state=True)(encoder_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 모델\n",
    "decoder_inputs = Input(shape=(3,))  \n",
    "decoder_embed = Embedding(len(tokenizer_ko.word_index)+1, 50)(decoder_inputs)\n",
    "decoder_mask = Masking(mask_value=0)(decoder_embed)\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_mask, initial_state=[h_state, c_state])\n",
    "decoder_dense = Dense(len(tokenizer_ko.word_index)+1, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"input_5:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 52).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"input_5:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 52).\n",
      "172/173 [============================>.] - ETA: 0s - loss: 4.4970 - acc: 0.3532WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"input_5:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 52).\n",
      "173/173 [==============================] - 69s 399ms/step - loss: 4.4969 - acc: 0.3532 - val_loss: 3.2912 - val_acc: 0.6607\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 68s 390ms/step - loss: 2.6553 - acc: 0.6609 - val_loss: 2.8736 - val_acc: 0.6629\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 65s 377ms/step - loss: 2.3468 - acc: 0.6632 - val_loss: 2.8205 - val_acc: 0.6636\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 66s 382ms/step - loss: 2.2129 - acc: 0.6635 - val_loss: 2.8229 - val_acc: 0.6639\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 68s 395ms/step - loss: 2.0924 - acc: 0.6647 - val_loss: 2.9239 - val_acc: 0.6648\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 69s 399ms/step - loss: 1.9870 - acc: 0.6660 - val_loss: 2.9026 - val_acc: 0.6654\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 69s 396ms/step - loss: 1.8925 - acc: 0.6670 - val_loss: 2.7962 - val_acc: 0.6663\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 69s 400ms/step - loss: 1.8079 - acc: 0.6688 - val_loss: 2.7867 - val_acc: 0.6672\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 71s 413ms/step - loss: 1.7434 - acc: 0.6710 - val_loss: 2.7399 - val_acc: 0.6671\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 70s 402ms/step - loss: 1.6872 - acc: 0.6733 - val_loss: 2.7554 - val_acc: 0.6475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x241616ca508>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 설정 및 학습\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_output_train, validation_data = ([encoder_input_test, decoder_input_test], decoder_output_test), batch_size = 128, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능평가를 위한 인코더 모델 생성\n",
    "encoder_model = Model(encoder_inputs, [h_state, c_state])\n",
    "encoder_h_state = Input(shape=(50,))\n",
    "encoder_c_state = Input(shape=(50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 모델 생성\n",
    "pd_decoder_outputs, pd_h_state, pd_c_state = decoder_lstm(decoder_mask, initial_state=[encoder_h_state, encoder_c_state])\n",
    "pd_decoder_softmax_outputs = decoder_dense(pd_decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + [encoder_h_state, encoder_c_state], [pd_decoder_softmax_outputs] + [pd_h_state, pd_c_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임상 신약 치료제 대한민국\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3) for input Tensor(\"input_6:0\", shape=(None, 3), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "화이브라더스코리아\n"
     ]
    }
   ],
   "source": [
    "# 키워드 입력\n",
    "input_stc = input()\n",
    "\n",
    "token_stc = input_stc.split()\n",
    "encode_stc = tokenizer_ko.texts_to_sequences([token_stc])\n",
    "pad_stc = pad_sequences(encode_stc, maxlen=50, padding=\"post\")\n",
    "\n",
    "# 인코더의 마지막 시점의 셀/은닉 상태 값\n",
    "states_value = encoder_model.predict(pad_stc)\n",
    "\n",
    "# <start> 를 정수 인코딩해서 numpy array 로\n",
    "predicted_seq = np.zeros((1,1))\n",
    "predicted_seq[0, 0] = en_to_index['<start>']\n",
    "\n",
    "# 각 시점마다 예측된 단어를 저장\n",
    "decoded_stc = []\n",
    "\n",
    "while True:\n",
    "    output_words, h, c = decoder_model.predict([predicted_seq] + states_value)\n",
    "    predicted_word = index_to_en[np.argmax(output_words[0,0])]  \n",
    "\n",
    "    # end 가 나오면 끝\n",
    "    if predicted_word == '<end>':\n",
    "        break\n",
    "\n",
    "    decoded_stc.append(predicted_word)\n",
    "\n",
    "    # 처음에는 <start>, 지금은 예측된 단어가 있으니 이것을 인풋으로 넣어주기 위해서 변수의 값을 업데이트\n",
    "    predicted_seq = np.zeros((1,1))\n",
    "    predicted_seq[0, 0] = np.argmax(output_words[0,0])\n",
    "\n",
    "    # 지금 시점의 상태 값을 다음 시점으로 넘기기 위해서 변수를 업데이트\n",
    "    states_value = [h, c]\n",
    "\n",
    "print(' '.join(decoded_stc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종목별 뉴스 분석 감정평가 + 나이브베이즈 모델을 이용한 감정평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def csv_processing():\n",
    "    df = pd.read_csv(\"종목별뉴스분석감정평가.csv\")\n",
    "    \n",
    "    # 필요없는 컬럼 삭제\n",
    "    del df['ITMT_CD_NM']\n",
    "    del df['ORG_TITL_NM']\n",
    "    del df['ORG_CTGO_NM']\n",
    "    del df['URL_ADDR']\n",
    "    del df['STRD_DTM']\n",
    "    del df['ZP']\n",
    "    del df['LOAD_DTM']\n",
    "    del df['CENTER_CD']\n",
    "    \n",
    "    # 모델을 학습할때 긍정과 부정 데이터 수가 비슷하면 \n",
    "    # 성능이 좋아지기 때문에 먼저 중립데이터 행을 제거한 후\n",
    "    # 긍정과 부정 비율을 비슷하게 맞춰준다 \n",
    "    \n",
    "    # 보기 쉽게 데이터 컬럼명 바꾸기\n",
    "    df.rename(columns={\"STRD_YYMMDD\":\"기준일자\",'ITMT_CD_NM.1':'주식종목명','ORG_CTNT':'원문내용','BEM_IDEX_VAL':'감성지표값','SCR_CTGO_NM':'스코어분류명'}, inplace=True)\n",
    "    \n",
    "    # 중립데이터 제거\n",
    "    df1 = df[df['스코어분류명']=='중립'].index\n",
    "    df1 = df.drop(df1)\n",
    "    \n",
    "    # 주식종목명당 기사가 14개 미만인 종목을 결측치로 취급하여 제거\n",
    "    df.groupby(df['주식종목명']).count().sort_values(by=['기준일자'],ascending=False)\n",
    "    xx =df.groupby(df['주식종목명']).count().sort_values(by=['기준일자'],ascending=False)\n",
    "    aa = xx[xx['기준일자']>14]\n",
    "    index_list = list(aa.index)\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        if df.iloc[i,1] not in index_list:\n",
    "            df.iloc[i,1] = np.nan\n",
    "        \n",
    "    df.dropna(inplace =True)\n",
    "    # 종목마다 긍정 부정인 판별이 다르기 때문에 원문 내용에 주식종목명 추가\n",
    "    df['원문내용'] = df['주식종목명'] + ' ' + df['원문내용']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_datas 생성\n",
    "def test_datas():\n",
    "    df = csv_processing()\n",
    "    \n",
    "    pos_doc = []\n",
    "    neg_doc = []\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        if df.iloc[i,4] == '긍정':\n",
    "            pos_doc.append(df.iloc[i,2])\n",
    "        else:\n",
    "            neg_doc.append(df.iloc[i,2])\n",
    "            \n",
    "    # pos_doc 길이 71871, neg_doc 길이 21094\n",
    "    # 학습시킬 데이터와 테스트 데이터 양을 같게 만들어줌\n",
    "    # 여기서는 test를 5000개, 대략 3:1 비율         \n",
    "         \n",
    "    train_datas = [[], []]\n",
    "    train_datas[0] = neg_doc[:16094]\n",
    "    train_datas[1] = pos_doc[:16094]\n",
    "    df = [neg_doc[16094:], pos_doc[16094:21094]]\n",
    "    \n",
    "    ## 오래 걸려서 test 10개만 \n",
    "    #train_datas = [[], []]\n",
    "    #train_datas[0] = neg_doc[:19332]\n",
    "    #train_datas[1] = pos_doc[:19332]\n",
    "    #test_datas = [neg_doc[19332:19342], pos_doc[19332:19342]]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정일때 정확도 :58.84\n",
      "긍정일때 정확도 :70.36\n"
     ]
    }
   ],
   "source": [
    "# 분류 시작 함수\n",
    "def start():\n",
    "    train_datas = open_csv()\n",
    "    df_t = test_datas()\n",
    "    df_tests = pd.DataFrame({'부정':df_t[0],'긍정':df_t[1]})\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(0,5000):\n",
    "        test_data = df_tests['부정'][i]\n",
    "        prob = naive_bayes(train_datas, test_data, 0.5, 0.5)\n",
    "        if prob[0] >= prob[1]:\n",
    "            count += 1\n",
    "    print(f'부정일때 정확도 :{count/5000*100}')\n",
    "    \n",
    "    # 오래걸려서 10개만 \n",
    "    #for i in range(0,10):\n",
    "    #    test_data = df_tests['부정'][i]\n",
    "    #    prob = naive_bayes(train_datas, test_data, 0.5, 0.5)\n",
    "    #    if prob[0] >= prob[1]:\n",
    "    #        count += 1\n",
    "    #print(f'부정일때 정확도 :{count/10*100}')\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(0,5000):\n",
    "        test_data = df_tests['긍정'][i]\n",
    "        prob = naive_bayes(train_datas, test_data, 0.5, 0.5)\n",
    "        if prob[0] <= prob[1]:\n",
    "            count += 1\n",
    "    print(f'긍정일때 정확도 :{count/5000*100}')\n",
    "    \n",
    "    # 오래걸려 10개만\n",
    "    #count = 0\n",
    "    #for i in range(0,10):\n",
    "    #    test_data = df_tests['긍정'][i]\n",
    "    #    prob = naive_bayes(train_datas, test_data, 0.5, 0.5)\n",
    "    #    if prob[0] <= prob[1]:\n",
    "    #        count += 1\n",
    "    #print(f'긍정일때 정확도 :{count/10*100}')\n",
    "    \n",
    "    train_datas = open_csv()\n",
    "    df_tests = test_datas()\n",
    "    test_dt= [' '.join(df_tests[0]), ' '.join(df_tests[1])]\n",
    "    \n",
    "    prob = naive_bayes(train_datas, test_dt[1], 0.5, 0.5)\n",
    "    print(f'긍정적인 데이터가 부정적일 확률 : {prob[0]}, 긍정적일 확률 : {prob[1]}')\n",
    "    \n",
    "    # 부정인 문장을 넣고, 실제로 부정이라고 나오는지 확인\n",
    "    prob = naive_bayes(train_datas, test_dt[0], 0.5, 0.5)\n",
    "    print(f'부정적인 데이터가 부정적일 확률 : {prob[0]}, 긍정적일 확률 : {prob[1]}')\n",
    "    \n",
    "# 전처리된 dataframe 불러와 train_set 생성    \n",
    "def open_csv():\n",
    "    df = csv_processing()\n",
    "    \n",
    "    pos_doc = []\n",
    "    neg_doc = []\n",
    "    \n",
    "    for i in range(0,len(df)):\n",
    "        if df.iloc[i,4] == '긍정':\n",
    "            pos_doc.append(df.iloc[i,2])\n",
    "        else:\n",
    "            neg_doc.append(df.iloc[i,2])\n",
    "            \n",
    "    train_datas = [[], []]\n",
    "    train_datas[0] = neg_doc[:16094]\n",
    "    train_datas[1] = pos_doc[:16094]\n",
    "    \n",
    "    #오래걸려 10개만\n",
    "    #train_datas = [[], []]\n",
    "    #train_datas[0] = neg_doc[:19332]\n",
    "    #train_datas[1] = pos_doc[:19332]\n",
    "    \n",
    "    # 리스트 형태는 토큰화하기가 어렵기 때문에, 전부 조인을 해서 하나의 문자열로 만들어준다\n",
    "    return [' '.join(train_datas[0]), ' '.join(train_datas[1])]\n",
    "\n",
    "# 스탑워드 제거, 토큰화, Bow화 함수\n",
    "# 단어 수가 늘어나면 단어별 확률이 소실될 수 있는데\n",
    "# 이를 방지하고 값을 간략하게 저장하게 위해서 로그를 취한다\n",
    "def calculate_doc_prob(train_data, test_data, nowords_weight):\n",
    "    # 스탑워드 제거\n",
    "    sw_train_data = re.compile('[^\\w]').sub(' ', train_data.lower())\n",
    "    # 토큰화\n",
    "    sw_train_token = sw_train_data.split()\n",
    "    # Bow화 (단어 : 빈도수 형태)\n",
    "    train_vector = dict(Counter(sw_train_token))\n",
    "\n",
    "    # 스탑워드 제거\n",
    "    sw_test_data = re.compile('[^\\w]').sub(' ', test_data.lower())\n",
    "    # 토큰화\n",
    "    sw_test_token = sw_test_data.split()\n",
    "    # Bow화 (단어 : 빈도수 형태)\n",
    "    test_vector = dict(Counter(sw_test_token))\n",
    "    total_wc = len(sw_train_token)\n",
    "    log_prob = 0\n",
    "\n",
    "    # log(P(test_data|긍정))\n",
    "    # 단어 10개로 이루어진 문장, 각 단어가 나올 확률이 (10/500000) -> 문장이 나올 확률은 0.00000000000000000000000000000000000000001024\n",
    "    # e.1024e-50 -> 0 -> (로그함수) -50\n",
    "    # 확률이 소실되는 것을 방지하고, 값을 간략하게 저장하게 위해서 로그를 취한다\n",
    "    for word in test_vector:\n",
    "        if word in train_vector:\n",
    "            log_prob += log(train_vector[word]/total_wc)\n",
    "        else:\n",
    "            # train 데이터셋에 없는 단어가 나왔으면... 해당 단어가 나올 확률 추정불가\n",
    "            # 해당 단어의 빈도수\n",
    "            # 빈도수가 없는 단어는 우리가 빈도를 지정하주자!\n",
    "            log_prob += log(nowords_weight/total_wc)\n",
    "\n",
    "    return log_prob\n",
    "\n",
    "# 긍정, 부정의 크기를 상대적인 크기로 정규화\n",
    "# 두 확률 값의 상대적인 비율을 return\n",
    "def naive_bayes(train_datas, test_data, pos_prob, neg_prob):\n",
    "    # P(긍정|test_data) = P(test_data|긍정) * P(긍정) / P(test_data)\n",
    "    test_pos_prob = calculate_doc_prob(train_datas[1], test_data, 0.1) + log(pos_prob) \n",
    "    # P(부정|test_data) = P(test_data|부정) * P(부정) / P(test_data)\n",
    "    test_neg_prob = calculate_doc_prob(train_datas[0], test_data, 0.1) + log(neg_prob) \n",
    "\n",
    "    # 10 : 5 -> 2 : 1\n",
    "    # 긍정, 부정의 상대적인 크기\n",
    "    # test_pos_prob (로그값) -> 로그값 -50 -> 지수함수 0에 수렴\n",
    "    # test_neg_prob (로그값) -> 로그값 -50 -> 지수함수 0에 수렴\n",
    "    # 로그함수 -> logex, 지수함수 -> e**x\n",
    "    # -50, -52 -> (지수화) e**-50=0, e**-52=0\n",
    "    # -50, -52 -> 0, -2 -> (지수화) e**0 = 1, e**-2 = 0.12\n",
    "    # 긍정 확률 1, 부정 확률 0.12 -> 1/1+0.12, 0.12/1+0.12\n",
    "    maxprob = max(test_neg_prob, test_pos_prob)\n",
    "    test_neg_prob -= maxprob\n",
    "    test_pos_prob -= maxprob\n",
    "    #print(test_neg_prob, test_pos_prob)\n",
    "    test_neg_prob = exp(test_neg_prob)\n",
    "    test_pos_prob = exp(test_pos_prob)\n",
    "    # 두 확률 값의 상대적인 비율\n",
    "    normalized_prob = [test_neg_prob/(test_neg_prob+test_pos_prob), test_pos_prob/(test_neg_prob+test_pos_prob)]\n",
    "\n",
    "    return normalized_prob\n",
    "\n",
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도 기반 종목 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_csv('종목별 핫이슈 키워드.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbp41\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "datas['ISSUE_CTNT'] = datas['ISSUE_CTNT'].apply(literal_eval)\n",
    "datas['ISSUE_CTNT'] = datas['ISSUE_CTNT'].apply(lambda x : [y['words'] for y in x])\n",
    "\n",
    "for i in range(0,len(datas)):\n",
    "    l = len(datas['ISSUE_CTNT'][i])\n",
    "    x_list = []\n",
    "    for j in range(0,l):\n",
    "        x = datas['ISSUE_CTNT'][i][j]\n",
    "        x_list = x_list + x\n",
    "    datas['ISSUE_CTNT'][i] = x_list\n",
    "    \n",
    "# 리스트 형태는 토큰화하기가 어렵기 때문에, 전부 조인을 해서 하나의 문자열로 만들어준다\n",
    "datas['ISSUE_CTNT'] = datas['ISSUE_CTNT'].apply(lambda x : ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('임상', 236), ('약품', 205), ('스펙트럼', 179), ('코호트', 285), ('신약', 191), ('포지오', 308), ('환자', 327), ('평가', 305), ('제약', 252), ('에페', 213), ('타이드', 291), ('레나', 102), ('사노', 146), ('경제', 32), ('잘탄', 241), ('패밀리', 300), ('매출', 112), ('아모', 196), ('개발', 27), ('기술', 68), ('바이오', 122), ('치료제', 283), ('밝혔', 128), ('병원', 136), ('임상 약품', 238), ('약품 스펙트럼', 207), ('스펙트럼 코호트', 180), ('코호트 신약', 286), ('신약 포지오', 193), ('포지오 환자', 309), ('환자 평가', 328), ('평가 약품', 307), ('약품 제약', 209), ('제약 에페', 254), ('에페 타이드', 215), ('타이드 레나', 293), ('레나 사노', 103), ('사노 경제', 147), ('경제 약품', 38), ('약품 신약', 208), ('신약 잘탄', 192), ('잘탄 패밀리', 242), ('패밀리 매출', 301), ('매출 아모', 113), ('아모 개발', 197), ('개발 경제', 28), ('경제 임상', 40), ('임상 기술', 237), ('기술 바이오', 70), ('바이오 제약', 123), ('제약 치료제', 255), ('치료제 밝혔', 284), ('밝혔 환자', 129), ('평가 병원', 306), ('병원 약품', 137), ('약품 사노', 206), ('사노 에페', 148), ('에페 레나', 214), ('레나 타이드', 104), ('타이드 경제', 292), ('경제 개발', 33), ('정세균', 248), ('중공업', 265), ('수산', 171), ('관련', 53), ('알루코', 203), ('한경', 312), ('총리', 279), ('주가', 258), ('급등', 66), ('서울', 159), ('per', 18), ('pbr', 13), ('업종', 210), ('차트', 274), ('정배', 246), ('분기', 138), ('이익', 229), ('영업', 222), ('매출액', 114), ('한국', 314), ('흑자', 334), ('전환', 245), ('정세균 중공업', 249), ('중공업 수산', 268), ('수산 관련', 173), ('관련 알루코', 54), ('알루코 한경', 204), ('한경 총리', 313), ('총리 경제', 280), ('경제 주가', 41), ('주가 급등', 260), ('급등 수산', 67), ('수산 중공업', 174), ('중공업 경제', 266), ('경제 서울', 37), ('서울 per', 160), ('per pbr', 20), ('pbr 업종', 15), ('업종 차트', 212), ('차트 정배', 275), ('정배 분기', 247), ('분기 이익', 140), ('이익 영업', 231), ('영업 매출액', 223), ('매출액 한국', 115), ('한국 중공업', 317), ('수산 경제', 172), ('경제 흑자', 45), ('흑자 전환', 335), ('수송관', 177), ('시험', 189), ('성능', 161), ('한난', 320), ('센터', 165), ('한국지역난방공사', 318), ('안전', 201), ('사업', 151), ('사장', 156), ('수송관 시험', 178), ('시험 성능', 190), ('성능 한난', 162), ('한난 센터', 321), ('센터 경제', 166), ('경제 한국지역난방공사', 43), ('한국지역난방공사 안전', 319), ('안전 사업', 202), ('사업 사장', 153), ('소송', 167), ('회사', 329), ('주장', 261), ('박모', 124), ('제기', 250), ('대응', 82), ('채권자', 277), ('포토', 310), ('지난', 270), ('공급', 48), ('이오', 227), ('동성', 88), ('bmp', 5), ('규모', 55), ('소송 회사', 168), ('회사 주장', 330), ('주장 박모', 262), ('박모 제기', 125), ('제기 대응', 251), ('대응 채권자', 83), ('채권자 포토', 278), ('포토 지난', 311), ('지난 주가', 271), ('주가 공급', 259), ('공급 이오', 49), ('이오 동성', 228), ('동성 제약', 89), ('제약 bmp', 253), ('bmp 규모', 6), ('글로벌', 62), ('해외', 322), ('세미나', 163), ('아마존', 194), ('진출', 272), ('kg', 9), ('시스', 181), ('가맹점', 21), ('시장', 184), ('렌탈', 105), ('비에스', 144), ('인수', 234), ('pe', 16), ('터스', 296), ('대신', 80), ('결제', 31), ('글로벌 해외', 63), ('해외 세미나', 323), ('세미나 아마존', 164), ('아마존 진출', 195), ('진출 kg', 273), ('kg 시스', 11), ('시스 가맹점', 183), ('가맹점 시장', 22), ('시장 렌탈', 186), ('렌탈 시스', 107), ('시스 kg', 182), ('kg 비에스', 10), ('비에스 경제', 145), ('경제 인수', 39), ('인수 pe', 235), ('pe 터스', 17), ('터스 대신', 297), ('대신 결제', 81), ('이타', 232), ('솔루션', 169), ('분기 영업', 139), ('영업 이익', 224), ('이익 매출액', 230), ('한국 이타', 315), ('이타 솔루션', 233), ('솔루션 경제', 170), ('두산', 90), ('산중', 157), ('공업', 50), ('그룹', 57), ('aws', 3), ('메카', 116), ('자회사', 239), ('개선', 29), ('데일리', 86), ('면세점', 118), ('매물', 110), ('금융', 64), ('기업', 72), ('가스', 23), ('터빈', 294), ('뉴스', 77), ('파워', 298), ('발전', 126), ('드론', 97), ('수소', 175), ('연료전지', 218), ('ces', 7), ('사랑', 149), ('열매', 220), ('회장', 331), ('로봇', 108), ('협동', 325), ('디지털', 99), ('중국', 269), ('두산 산중', 93), ('산중 공업', 158), ('공업 그룹', 52), ('그룹 aws', 58), ('aws 메카', 4), ('메카 자회사', 117), ('자회사 개선', 240), ('개선 데일리', 30), ('데일리 두산', 87), ('두산 면세점', 92), ('면세점 매물', 119), ('매물 경제', 111), ('경제 사업', 36), ('사업 금융', 152), ('금융 시장', 65), ('시장 기업', 185), ('기업 두산', 73), ('공업 가스', 51), ('가스 터빈', 24), ('터빈 경제', 295), ('경제 뉴스', 34), ('뉴스 파워', 79), ('파워 발전', 299), ('발전 중공업', 127), ('중공업 두산', 267), ('두산 드론', 91), ('드론 기술', 98), ('기술 수소', 71), ('수소 그룹', 176), ('그룹 연료전지', 61), ('연료전지 ces', 219), ('ces 경제', 8), ('경제 사랑', 35), ('사랑 열매', 150), ('열매 두산', 221), ('두산 회장', 94), ('회장 사업', 333), ('사업 시장', 154), ('시장 로봇', 187), ('로봇 협동', 109), ('협동 디지털', 326), ('디지털 중국', 100), ('제작', 256), ('드라마', 95), ('키이스트', 289), ('콘텐츠', 287), ('엔터', 216), ('ott', 12), ('제작 드라마', 257), ('드라마 키이스트', 96), ('키이스트 콘텐츠', 290), ('콘텐츠 사업', 288), ('사업 엔터', 155), ('엔터 ott', 217), ('aj', 0), ('데이터', 84), ('웍스', 225), ('네트', 75), ('주주', 263), ('배당', 130), ('취득', 281), ('배당금', 132), ('현금', 324), ('per aj', 19), ('aj pbr', 1), ('pbr 렌탈', 14), ('렌탈 데이터', 106), ('데이터 기업', 85), ('기업 업종', 74), ('업종 기술', 211), ('기술 aj', 69), ('aj 웍스', 2), ('웍스 네트', 226), ('네트 주주', 76), ('주주 배당', 264), ('배당 취득', 131), ('취득 배당금', 282), ('배당금 규모', 133), ('규모 경제', 56), ('경제 현금', 44), ('고객', 46), ('퍼시픽', 302), ('아모레', 198), ('변화', 134), ('브랜드', 142), ('강화', 25), ('밀크', 120), ('장애', 243), ('뷰티', 141), ('고객 퍼시픽', 47), ('퍼시픽 아모레', 304), ('아모레 그룹', 200), ('그룹 변화', 59), ('변화 디지털', 135), ('디지털 회장', 101), ('회장 브랜드', 332), ('브랜드 강화', 143), ('강화 밀크', 26), ('밀크 아모레', 121), ('아모레 경제', 199), ('경제 퍼시픽', 42), ('퍼시픽 그룹', 303), ('그룹 시장', 60), ('시장 차트', 188), ('차트 한국', 276), ('한국 장애', 316), ('장애 뉴스', 244), ('뉴스 뷰티', 78)])\n",
      "(10, 336)\n"
     ]
    }
   ],
   "source": [
    "# 10개만 (컴퓨팅 파워 문제)\n",
    "stock = datas[:10]\n",
    "\n",
    "# ngram_range=(1, 2) 는 단어를 1개 혹은 2개 연속으로 보겠다\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 2))\n",
    "# ketwords 대신 ISSUE_CTNT사용\n",
    "tfidf_matrix = tfidf_vec.fit_transform(stock['ISSUE_CTNT'])\n",
    "print(tfidf_vec.vocabulary_.items())\n",
    "# 10은 종목의 개수, 336은 단어의 개수 -> 하나의 주식를 336개 열을 가진 벡터로 표현\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.02148812 0.01348856 0.0252953  0.00904519 0.01455037\n",
      "  0.02083305 0.         0.01884108 0.008866  ]\n",
      " [0.02148812 1.         0.01768577 0.01658319 0.01185977 0.24681774\n",
      "  0.04627423 0.         0.05956552 0.03671185]\n",
      " [0.01348856 0.01768577 1.         0.         0.00744464 0.01197566\n",
      "  0.03752858 0.03705548 0.00801132 0.00729715]\n",
      " [0.0252953  0.01658319 0.         1.         0.         0.\n",
      "  0.         0.         0.02253561 0.        ]\n",
      " [0.00904519 0.01185977 0.00744464 0.         1.         0.00803067\n",
      "  0.02516602 0.         0.02507347 0.01862875]\n",
      " [0.01455037 0.24681774 0.01197566 0.         0.00803067 1.\n",
      "  0.01409909 0.         0.00864196 0.02996675]\n",
      " [0.02083305 0.04627423 0.03752858 0.         0.02516602 0.01409909\n",
      "  1.         0.02908391 0.02978625 0.09817955]\n",
      " [0.         0.         0.03705548 0.         0.         0.\n",
      "  0.02908391 1.         0.         0.        ]\n",
      " [0.01884108 0.05956552 0.00801132 0.02253561 0.02507347 0.00864196\n",
      "  0.02978625 0.         1.         0.00526582]\n",
      " [0.008866   0.03671185 0.00729715 0.         0.01862875 0.02996675\n",
      "  0.09817955 0.         0.00526582 1.        ]]\n",
      "[[0 3 1 6 8 5 2 4 9 7]\n",
      " [1 5 8 6 9 0 2 3 4 7]\n",
      " [2 6 7 1 0 5 8 4 9 3]\n",
      " [3 0 8 1 2 4 5 6 7 9]\n",
      " [4 6 8 9 1 0 5 2 3 7]\n",
      " [5 1 9 0 6 2 8 4 3 7]\n",
      " [6 9 1 2 8 7 4 0 5 3]\n",
      " [7 2 6 0 1 3 4 5 8 9]\n",
      " [8 1 6 4 3 0 5 2 9 7]\n",
      " [9 6 1 5 4 0 2 8 3 7]]\n"
     ]
    }
   ],
   "source": [
    "# 유사도 행렬 (10, 10)\n",
    "# 1, 1 / 1, 2 / .... / 1, 10 -> 1번째 주식와 1~10번재 주식의 유사도\n",
    "# 2, 1 / 2, 2 / .... / 2, 10 -> 2번째 주식와 1~10번째 주식의 유사도\n",
    "\n",
    "genres_similarity = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(genres_similarity)\n",
    "\n",
    "# 유사도 값이 높은 순으로 인덱스 값을 뽑아낸다\n",
    "similar_index = np.argsort(-genres_similarity)\n",
    "print(similar_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한미약품\n",
      "   STRD_DATE  STCK_ITMT_CD_NM ITMT_CD_NM  \\\n",
      "3   20200102            66430       와이오엠   \n",
      "1   20200102            17550      수산중공업   \n",
      "\n",
      "                                          ISSUE_CTNT  ISSUE_CTGO_USE_DOC_NUM  \\\n",
      "3  소송 회사 주장 엠 박모 제기 법 대응 채권자 씨 엠 포토 지난 주가 공급 이오 동...                       6   \n",
      "1  정세균 중공업 수산 관련 알루코 한경 총리 경제 주가 급등 수산 중공업 배 경제 서...                      13   \n",
      "\n",
      "        ZP LOAD_DTM CENTER_CD  \n",
      "3  46729.0   ZZZZZZ     BBP14  \n",
      "1  18628.0   ZZZZZZ     BBP14  \n"
     ]
    }
   ],
   "source": [
    "# 사용자가 입력한 주식의 인덱스 값을 찾아내고\n",
    "# similar_index 에 기록된 유사한 주식 인덱스를 찾아내고\n",
    "# 유사한 인덱스를 토대로 주식을 찾아내면 된다!\n",
    "input_stock = input()\n",
    "\n",
    "stock_index = stock[stock['ITMT_CD_NM'] ==input_stock].index.values\n",
    "#print(stock_index)\n",
    "similar_stock = similar_index[stock_index, 1:3]\n",
    "#print(similar_stock)\n",
    "# 인덱스로 사용하기 위해서는 1차원으로 변형해줘야하기 때문\n",
    "similar_stock_index = similar_stock.reshape(-1)\n",
    "#print(similar_stock_index)\n",
    "print(stock.iloc[similar_stock_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1순위 와이오엠, 2순위 수산중공업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
